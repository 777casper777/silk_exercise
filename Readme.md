
---

# Scalable Host Ingestion Pipeline

This project implements a modular, scalable host ingestion and normalization pipeline that fetches host data from multiple sources (e.g., Qualys, Crowdstrike), normalizes the structure, deduplicates, and persists it into MongoDB. 
The system is designed for easy extension and background task processing.


## ğŸ“š Table of Contents

- [Tech Stack](#-tech-stack)
- [Project Structure](#-project-structure)
- [Event Loop Flow](#-event-loop-flow)
- [Celery + Redis Event Loop](#-celery--redis-event-loop)
- [Getting Started](#-getting-started)
- [Adding New Vendor](#-adding-new-vendor)
- [Visualization](#-visualization)

## ğŸ”§ Tech Stack

* **Python 3.10+**
* **Celery + Redis** â€” background task queue
* **MongoDB** â€” database for storing processed hosts
* **Pydantic** â€” data modeling and validation
* **Docker + Docker Compose** â€” local orchestration

## ğŸ“ Project Structure

```
project/
â”‚
â”œâ”€â”€ app/                          # Application core
â”‚   â”œâ”€â”€ main.py                   # Entry point
â”‚   â”‚
â”‚   â”œâ”€â”€ config.py                 # ENV-based config (fetchers, DB, etc.)
â”‚   â”œâ”€â”€ loader.py                 # Dynamically loads fetchers based on config
â”‚
â”‚   â”œâ”€â”€ clients/                  # Vendor-specific API clients
â”‚       â”œâ”€â”€ base.py               # Abstract base client
â”‚       â”œâ”€â”€ crowdstrike.py        # Crowdstrike API client
â”‚       â””â”€â”€ qualys.py             # Qualys API client

â”‚   â”œâ”€â”€ models/                   # Pydantic data models
â”‚   â”‚   â”œâ”€â”€ raw_qualys.py         # Raw Qualys host schema
â”‚   â”‚   â”œâ”€â”€ raw_crowdstrike.py    # Raw Crowdstrike host schema
â”‚   â”‚   â””â”€â”€ unified_host.py       # Final unified schema
â”‚
â”‚   â”œâ”€â”€ services/                 # Business logic
â”‚   â”‚   â”œâ”€â”€ normalizer.py         # Vendor-specific normalization
â”‚   â”‚   â”œâ”€â”€ deduplicator.py       # Host deduplication logic
â”‚   â”‚   â””â”€â”€ persistence.py        # MongoDB insert/update logic
â”‚
â”‚   â”œâ”€â”€ tasks/                    # Background tasks (e.g., Celery)
â”‚   â”‚   â””â”€â”€ fetch_and_process.py  # Async pipeline task
â”‚
â”‚   â””â”€â”€ utils/                    # General utilities
â”‚       â”œâ”€â”€ pagination.py         # Pagination helper for APIs
â”‚       
â”‚
â”œâ”€â”€ tests/                        # Pytest test cases
â”‚   â”œâ”€â”€ test_normalizer.py        # Tests for normalize functions
â”‚   â”œâ”€â”€ test_deduplicator.py      # Tests for deduplication logic
â”‚   â”œâ”€â”€ test_persistence.py       # Tests MongoDB upsert
â”‚   â””â”€â”€ test_upsert_real.py       # Real upsert tests against live DB
â”‚
â”œâ”€â”€ visualisations/               # Data analysis / reporting
â”‚   â”œâ”€â”€ plot_os_distribution.py   # Pie chart: OS breakdown
â”‚   â”œâ”€â”€ plot_host_age.py          # Bar chart: host age analysis
â”‚   â””â”€â”€ screenshots/              # Saved plots for review
â”‚       â””â”€â”€ *.png
â”‚
â”œâ”€â”€ .env                          # Local environment variables
â”œâ”€â”€ docker-compose.yml            # Services: MongoDB, Redis, API, Celery
â”œâ”€â”€ Dockerfile                    # Build app container
â”œâ”€â”€ pytest.ini                    # Pytest config (markers, paths)
â”œâ”€â”€ requirements.txt              # All dependencies
â””â”€â”€ README.md                     # Setup + usage instructions
```

## ğŸ” Event Loop Flow

The project uses `asyncio` to coordinate asynchronous data ingestion, normalization, deduplication, and MongoDB persistence.

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         main.py entry        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
asyncio.run(_run_async_pipeline())
             â†“
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Function: _run_async_pipeline (in fetch_and_process.py)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
             â†“
fetchers = load_fetchers()
             â†“
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Function: load_fetchers() (in loader.py)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
             â†“
Reads config.py:
ACTIVE_FETCHERS = [
  "app.clients.real_qualys.QualysFetcher",
  "app.clients.real_crowdstrike.CrowdstrikeFetcher"
]
             â†“
Uses import_module(...) and getattr(...) to create:
- fetcher1 = QualysFetcher()
- fetcher2 = CrowdstrikeFetcher()
             â†“
Returns list of fetchers
             â†“
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Loop:
for fetcher in fetchers:
    async for raw_host in fetcher.fetch_hosts():
        process_host(raw_host)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
             â†“
â†’ First: QualysFetcher.fetch_hosts()
â†’ Then:  CrowdstrikeFetcher.fetch_hosts()

Both internally call:

    async for host in paginated_fetch(self._fetch_page, batch_size=2):
        yield host

             â†“
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Function: paginated_fetch(...) (in pagination.py)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- skip = 0
- calls self._fetch_page(skip, limit)
- yields host one by one
- increments skip and loops again

             â†“
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Function: _fetch_page(self, skip, limit)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- Sends POST to API with:
    skip=X, limit=2
- Parses JSON list of hosts
- Yields them one by one

             â†“
ğŸ¯ Final result: raw host dicts from all sources

             â†“
Then (per host):
- normalize_* â†’ convert to UnifiedHost
- Check Redis:
    â†’ if key exists â†’ skip
    â†’ else:
        - store key in Redis (with TTL if enabled)
        - upsert_host() â†’ insert/update in MongoDB

```

## ğŸ”„ Celery + Redis Event Loop

Hereâ€™s how they interact:

```text
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚ Docker Compose starts all project services          â”‚
â”‚ worker: celery -A app.tasks.fetch_and_process ...   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“
Celery worker starts with module app.tasks.fetch_and_process
                             â†“
@task â†’ run_pipeline() is triggered directly (e.g., inside main.py)
                             â†“
asyncio.run(_run_async_pipeline())
                             â†“
Main pipeline steps:
â†’ load_fetchers()          # Dynamically load all active fetchers
â†’ fetch_hosts()            # Asynchronously collect data page-by-page
â†’ normalize                # Convert raw data to UnifiedHost objects
â†’ Redis check              # Perform deduplication via Redis key
â†’ if new:
    â†’ Store Redis key
    â†’ upsert_host()        # Insert or update in MongoDB

```
## ğŸš€ Getting Started

1. **Clone the repository**
2. **Create a `.env` file** from the example and set your API token
3. **Start services**:

```bash
docker-compose up --build
```


## âœ… Adding New Vendor

1. Create a new fetcher in `app/clients/`
2. Inherit from `BaseHostFetcher` and implement `fetch_hosts()`
3. Register it in `ACTIVE_FETCHERS` in `config.py`

## ğŸ“Š Visualization

* Run scripts in `visualizations/` to generate analytics
* Export screenshots to `visualizations/screenshots/`

### ğŸ³ Run from Docker

To run visualization scripts inside the Docker container :

```bash
docker-compose run --rm api python visualisations/plot_host_age.py
```

```bash
docker-compose run --rm  api python visualisations/plot_os_distribution.py
```


### ğŸ•’ Host Age Distribution

![Host Age Distribution](visualisations/screenshots/host_age_chart.png)

### ğŸ’» OS Distribution

![OS Distribution](visualisations/screenshots/os_distribution.png)
